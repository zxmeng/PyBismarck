# shuffle: order des

### dense-logit-spec

python bismarck_front.py dense-logit-spec.py

python bismarck_front.py dense-logit-spec.py
A shuffled table __bismarck_shuffled_forest_1 is created for training
iteration 1 	loss: 372386.415792 	improvement:  None
iteration 2 	loss: 373259.801096 	improvement:  -0.00234537369529
iteration 3 	loss: 373381.155009 	improvement:  -0.00032511915967
iteration 4 	loss: 373405.930767 	improvement:  -6.63551393091e-05
iteration 5 	loss: 373411.768446 	improvement:  -1.56336004387e-05
iteration 6 	loss: 373413.213579 	improvement:  -3.87007751253e-06
iteration 7 	loss: 373413.57167 	improvement:  -9.58968603695e-07
iteration 8 	loss: 373413.661854 	improvement:  -2.41512718933e-07
iteration 9 	loss: 373413.684421 	improvement:  -6.04326094579e-08
iteration 10 	loss: 373413.690145 	improvement:  -1.53293042818e-08
iteration 11 	loss: 373413.691588 	improvement:  -3.8635435188e-09
iteration 12 	loss: 373413.691952 	improvement:  -9.74790121273e-10
iteration 13 	loss: 373413.692044 	improvement:  -2.46000654494e-10
iteration 14 	loss: 373413.692067 	improvement:  -6.21347893452e-11
iteration 15 	loss: 373413.692073 	improvement:  -1.57038011942e-11
iteration 16 	loss: 373413.692074 	improvement:  -3.87501656572e-12
iteration 17 	loss: 373413.692074 	improvement:  -1.03098916149e-12
iteration 18 	loss: 373413.692075 	improvement:  -2.51745917117e-13
iteration 19 	loss: 373413.692075 	improvement:  -7.23282387258e-14
iteration 20 	loss: 373413.692075 	improvement:  -1.0755707914e-14
iteration 21 	loss: 373413.692075 	improvement:  -5.29991404456e-15
iteration 22 	loss: 373413.692075 	improvement:  -2.64995702228e-15
iteration 23 	loss: 373413.692075 	improvement:  0.0
iteration 24 	loss: 373413.692075 	improvement:  0.0
iteration 25 	loss: 373413.692075 	improvement:  0.0
iteration 26 	loss: 373413.692075 	improvement:  0.0
iteration 27 	loss: 373413.692075 	improvement:  0.0
iteration 28 	loss: 373413.692075 	improvement:  0.0
iteration 29 	loss: 373413.692075 	improvement:  0.0
iteration 30 	loss: 373413.692075 	improvement:  0.0
iteration 31 	loss: 373413.692075 	improvement:  0.0
iteration 32 	loss: 373413.692075 	improvement:  0.0
iteration 33 	loss: 373413.692075 	improvement:  0.0
iteration 34 	loss: 373413.692075 	improvement:  0.0
iteration 35 	loss: 373413.692075 	improvement:  0.0
iteration 36 	loss: 373413.692075 	improvement:  0.0
iteration 37 	loss: 373413.692075 	improvement:  0.0
iteration 38 	loss: 373413.692075 	improvement:  0.0
iteration 39 	loss: 373413.692075 	improvement:  0.0
iteration 40 	loss: 373413.692075 	improvement:  0.0
iteration 41 	loss: 373413.692075 	improvement:  0.0
iteration 42 	loss: 373413.692075 	improvement:  0.0
iteration 43 	loss: 373413.692075 	improvement:  0.0
iteration 44 	loss: 373413.692075 	improvement:  0.0
iteration 45 	loss: 373413.692075 	improvement:  0.0
iteration 46 	loss: 373413.692075 	improvement:  0.0
iteration 47 	loss: 373413.692075 	improvement:  0.0
iteration 48 	loss: 373413.692075 	improvement:  0.0
iteration 49 	loss: 373413.692075 	improvement:  0.0
iteration 50 	loss: 373413.692075 	improvement:  0.0
iteration 51 	loss: 373413.692075 	improvement:  0.0
iteration 52 	loss: 373413.692075 	improvement:  0.0
iteration 53 	loss: 373413.692075 	improvement:  0.0
iteration 54 	loss: 373413.692075 	improvement:  0.0
iteration 55 	loss: 373413.692075 	improvement:  0.0
iteration 56 	loss: 373413.692075 	improvement:  0.0
iteration 57 	loss: 373413.692075 	improvement:  0.0
iteration 58 	loss: 373413.692075 	improvement:  0.0
iteration 59 	loss: 373413.692075 	improvement:  0.0
iteration 60 	loss: 373413.692075 	improvement:  0.0
iteration 61 	loss: 373413.692075 	improvement:  0.0
iteration 62 	loss: 373413.692075 	improvement:  0.0
iteration 63 	loss: 373413.692075 	improvement:  0.0
iteration 64 	loss: 373413.692075 	improvement:  0.0
iteration 65 	loss: 373413.692075 	improvement:  0.0
iteration 66 	loss: 373413.692075 	improvement:  0.0
iteration 67 	loss: 373413.692075 	improvement:  0.0
iteration 68 	loss: 373413.692075 	improvement:  0.0
iteration 69 	loss: 373413.692075 	improvement:  0.0
iteration 70 	loss: 373413.692075 	improvement:  0.0
iteration 71 	loss: 373413.692075 	improvement:  0.0
iteration 72 	loss: 373413.692075 	improvement:  0.0
iteration 73 	loss: 373413.692075 	improvement:  0.0
iteration 74 	loss: 373413.692075 	improvement:  0.0
iteration 75 	loss: 373413.692075 	improvement:  0.0
iteration 76 	loss: 373413.692075 	improvement:  0.0
iteration 77 	loss: 373413.692075 	improvement:  0.0
iteration 78 	loss: 373413.692075 	improvement:  0.0
iteration 79 	loss: 373413.692075 	improvement:  0.0
iteration 80 	loss: 373413.692075 	improvement:  0.0
iteration 81 	loss: 373413.692075 	improvement:  0.0
iteration 82 	loss: 373413.692075 	improvement:  0.0
iteration 83 	loss: 373413.692075 	improvement:  0.0
iteration 84 	loss: 373413.692075 	improvement:  0.0
iteration 85 	loss: 373413.692075 	improvement:  0.0
iteration 86 	loss: 373413.692075 	improvement:  0.0
iteration 87 	loss: 373413.692075 	improvement:  0.0
iteration 88 	loss: 373413.692075 	improvement:  0.0
iteration 89 	loss: 373413.692075 	improvement:  0.0
iteration 90 	loss: 373413.692075 	improvement:  0.0
iteration 91 	loss: 373413.692075 	improvement:  0.0
iteration 92 	loss: 373413.692075 	improvement:  0.0
iteration 93 	loss: 373413.692075 	improvement:  0.0
iteration 94 	loss: 373413.692075 	improvement:  0.0
iteration 95 	loss: 373413.692075 	improvement:  0.0
iteration 96 	loss: 373413.692075 	improvement:  0.0
iteration 97 	loss: 373413.692075 	improvement:  0.0
iteration 98 	loss: 373413.692075 	improvement:  0.0
iteration 99 	loss: 373413.692075 	improvement:  0.0
iteration 100 	loss: 373413.692075 	improvement:  0.0
--- Execution Time ---
--- 115.095814943 seconds ---



### dense-svm-spec
python bismarck_front.py dense-svm-spec.py
A shuffled table __bismarck_shuffled_forest_12 is created for training
iteration 1 	loss: 523108.622107 	improvement:  None
iteration 2 	loss: 523200.719523 	improvement:  -0.000176057919908
iteration 3 	loss: 523291.825309 	improvement:  -0.000174131614379
iteration 4 	loss: 523202.41135 	improvement:  0.000170868251174
iteration 5 	loss: 523151.793294 	improvement:  9.67466029646e-05
iteration 6 	loss: 523145.146745 	improvement:  1.27048182367e-05
iteration 7 	loss: 523128.744072 	improvement:  3.13539620143e-05
iteration 8 	loss: 523061.424046 	improvement:  0.000128687301017
iteration 9 	loss: 523010.368944 	improvement:  9.7608234557e-05
iteration 10 	loss: 523067.573966 	improvement:  -0.000109376458319
iteration 11 	loss: 523047.201 	improvement:  3.89490131869e-05
iteration 12 	loss: 523080.798748 	improvement:  -6.42346384414e-05
iteration 13 	loss: 523057.815307 	improvement:  4.39386061638e-05
iteration 14 	loss: 523025.310695 	improvement:  6.21434401356e-05
iteration 15 	loss: 523016.427181 	improvement:  1.69848635781e-05
iteration 16 	loss: 523038.538291 	improvement:  -4.22761282875e-05
iteration 17 	loss: 523016.893691 	improvement:  4.138241836e-05
iteration 18 	loss: 522993.074679 	improvement:  4.55415735158e-05
iteration 19 	loss: 522970.268013 	improvement:  4.36079689181e-05
iteration 20 	loss: 523021.668073 	improvement:  -9.82848600309e-05
iteration 21 	loss: 523009.650878 	improvement:  2.29764752698e-05
iteration 22 	loss: 522997.462032 	improvement:  2.33052027425e-05
iteration 23 	loss: 522954.521033 	improvement:  8.21055591726e-05
iteration 24 	loss: 522980.213655 	improvement:  -4.91297437897e-05
iteration 25 	loss: 523033.577337 	improvement:  -0.000102037668774
iteration 26 	loss: 522993.777608 	improvement:  7.60940227484e-05
iteration 27 	loss: 523026.916725 	improvement:  -6.33642673314e-05
iteration 28 	loss: 523005.655598 	improvement:  4.06501581221e-05
iteration 29 	loss: 523016.545503 	improvement:  -2.08217715916e-05
iteration 30 	loss: 522961.907888 	improvement:  0.000104466322197
iteration 31 	loss: 522983.201252 	improvement:  -4.07168552773e-05
iteration 32 	loss: 522995.744423 	improvement:  -2.39838886755e-05
iteration 33 	loss: 523030.582689 	improvement:  -6.66129057177e-05
iteration 34 	loss: 522998.006139 	improvement:  6.22842163827e-05
iteration 35 	loss: 522986.679555 	improvement:  2.16570306714e-05
iteration 36 	loss: 522951.387012 	improvement:  6.74826804251e-05
iteration 37 	loss: 523033.726043 	improvement:  -0.000157450640132
iteration 38 	loss: 522997.574755 	improvement:  6.91184652115e-05
iteration 39 	loss: 522999.463927 	improvement:  -3.61220039285e-06
iteration 40 	loss: 522994.003719 	improvement:  1.04401777356e-05
iteration 41 	loss: 522997.917304 	improvement:  -7.48303879806e-06
iteration 42 	loss: 523029.272413 	improvement:  -5.9952646354e-05
iteration 43 	loss: 523005.488024 	improvement:  4.54742981578e-05
iteration 44 	loss: 522993.494506 	improvement:  2.29319153138e-05
iteration 45 	loss: 523023.104089 	improvement:  -5.66155862932e-05
iteration 46 	loss: 522987.154183 	improvement:  6.87348346428e-05
iteration 47 	loss: 522977.885635 	improvement:  1.77223236802e-05
iteration 48 	loss: 523002.23376 	improvement:  -4.65567012885e-05
iteration 49 	loss: 523013.522577 	improvement:  -2.15846431149e-05
iteration 50 	loss: 522994.57684 	improvement:  3.62241829191e-05
iteration 51 	loss: 523011.209506 	improvement:  -3.18027504921e-05
iteration 52 	loss: 523010.853226 	improvement:  6.81208542929e-07
iteration 53 	loss: 523025.145135 	improvement:  -2.73262188688e-05
iteration 54 	loss: 523014.474651 	improvement:  2.04014738035e-05
iteration 55 	loss: 522988.366429 	improvement:  4.99187376418e-05
iteration 56 	loss: 522999.68436 	improvement:  -2.16408853437e-05
iteration 57 	loss: 523035.13234 	improvement:  -6.77782048193e-05
iteration 58 	loss: 523006.853605 	improvement:  5.40666073338e-05
iteration 59 	loss: 523040.930707 	improvement:  -6.51561300472e-05
iteration 60 	loss: 523001.839065 	improvement:  7.47391645281e-05
iteration 61 	loss: 523024.350786 	improvement:  -4.30432920298e-05
iteration 62 	loss: 523004.435332 	improvement:  3.80774882776e-05
iteration 63 	loss: 523002.03853 	improvement:  4.58275686606e-06
iteration 64 	loss: 522988.09855 	improvement:  2.66537771921e-05
iteration 65 	loss: 522974.471212 	improvement:  2.60566899204e-05
iteration 66 	loss: 523059.910181 	improvement:  -0.000163371203958
iteration 67 	loss: 523012.836878 	improvement:  8.99960059216e-05
iteration 68 	loss: 522974.638499 	improvement:  7.30352609088e-05
iteration 69 	loss: 523022.420502 	improvement:  -9.13658136773e-05
iteration 70 	loss: 523002.406061 	improvement:  3.8266889788e-05
iteration 71 	loss: 523020.479239 	improvement:  -3.45565860607e-05
iteration 72 	loss: 522974.740615 	improvement:  8.74509222537e-05
iteration 73 	loss: 523003.515271 	improvement:  -5.50211184639e-05
iteration 74 	loss: 523029.519878 	improvement:  -4.97216678908e-05
iteration 75 	loss: 522962.965616 	improvement:  0.00012724761999
iteration 76 	loss: 523026.922814 	improvement:  -0.000122297756927
iteration 77 	loss: 522973.334724 	improvement:  0.000102457611671
iteration 78 	loss: 523030.111951 	improvement:  -0.000108566198456
iteration 79 	loss: 522985.906099 	improvement:  8.45187525401e-05
iteration 80 	loss: 523000.205036 	improvement:  -2.73409608237e-05
iteration 81 	loss: 522984.333185 	improvement:  3.03476947756e-05
iteration 82 	loss: 522991.736543 	improvement:  -1.41559849316e-05
iteration 83 	loss: 523004.150012 	improvement:  -2.37354969087e-05
iteration 84 	loss: 522993.08088 	improvement:  2.1164520304e-05
iteration 85 	loss: 522999.377916 	improvement:  -1.20403804681e-05
iteration 86 	loss: 522994.121676 	improvement:  1.00501838204e-05
iteration 87 	loss: 523001.800935 	improvement:  -1.4683261042e-05
iteration 88 	loss: 523015.494667 	improvement:  -2.61829532184e-05
iteration 89 	loss: 522959.148967 	improvement:  0.00010773237295
iteration 90 	loss: 522990.631346 	improvement:  -6.02004558275e-05
iteration 91 	loss: 522990.969901 	improvement:  -6.47345376252e-07
iteration 92 	loss: 523041.471229 	improvement:  -9.65625224801e-05
iteration 93 	loss: 523026.12334 	improvement:  2.93435396088e-05
iteration 94 	loss: 523047.795805 	improvement:  -4.14366777429e-05
iteration 95 	loss: 522996.624579 	improvement:  9.78327925925e-05
iteration 96 	loss: 522968.550441 	improvement:  5.36793858404e-05
iteration 97 	loss: 522988.034974 	improvement:  -3.72575603171e-05
iteration 98 	loss: 523014.832194 	improvement:  -5.12386876333e-05
iteration 99 	loss: 523002.903336 	improvement:  2.28078770385e-05
iteration 100 	loss: 522991.146669 	improvement:  2.2479161976e-05
--- Execution Time ---
--- 98.4180829525 seconds ---



### Sparse-logit

python bismarck_front.py sparse-logit-spec.py
A shuffled table __bismarck_shuffled_dblife_22 is created for training
iteration 1 	loss: 36024.9708853 	improvement:  None
iteration 2 	loss: 32450.841585 	improvement:  0.0992125520833
iteration 3 	loss: 30099.7616957 	improvement:  0.0724505058893
iteration 4 	loss: 28140.6118984 	improvement:  0.065088548443
iteration 5 	loss: 26390.2127913 	improvement:  0.0622018850705
iteration 6 	loss: 24780.4175481 	improvement:  0.0609997068208
iteration 7 	loss: 23243.8681547 	improvement:  0.0620065981708
iteration 8 	loss: 21767.4878958 	improvement:  0.0635169778563
iteration 9 	loss: 20337.0797107 	improvement:  0.0657130575617
iteration 10 	loss: 18974.1715603 	improvement:  0.0670159221384
iteration 11 	loss: 17667.9349046 	improvement:  0.0688428821026
iteration 12 	loss: 16392.0333737 	improvement:  0.0722156572202
iteration 13 	loss: 15187.8176546 	improvement:  0.0734634740935
iteration 14 	loss: 14033.3615097 	improvement:  0.0760119834948
iteration 15 	loss: 12935.7966459 	improvement:  0.0782111159226
iteration 16 	loss: 11903.1149977 	improvement:  0.0798313143387
iteration 17 	loss: 10951.2892585 	improvement:  0.0799644243878
iteration 18 	loss: 10050.8710975 	improvement:  0.082220288381
iteration 19 	loss: 9226.70826011 	improvement:  0.0819991450871
iteration 20 	loss: 8493.84622047 	improvement:  0.0794283312072
iteration 21 	loss: 7837.12262367 	improvement:  0.0773175755424
iteration 22 	loss: 7243.18242348 	improvement:  0.0757854928035
iteration 23 	loss: 6724.99266657 	improvement:  0.0715417238732
iteration 24 	loss: 6280.28886185 	improvement:  0.0661270319204
iteration 25 	loss: 5902.05579711 	improvement:  0.0602254248268
iteration 26 	loss: 5583.35947391 	improvement:  0.0539975110625
iteration 27 	loss: 5316.45094098 	improvement:  0.0478042895465
iteration 28 	loss: 5095.01265875 	improvement:  0.041651523674
iteration 29 	loss: 4912.01202641 	improvement:  0.0359176011127
iteration 30 	loss: 4761.04267765 	improvement:  0.0307347270204
iteration 31 	loss: 4636.5271223 	improvement:  0.0261530013046
iteration 32 	loss: 4533.9414126 	improvement:  0.0221255493573
iteration 33 	loss: 4449.88566775 	improvement:  0.0185392216631
iteration 34 	loss: 4380.70428495 	improvement:  0.015546777595
iteration 35 	loss: 4323.66327338 	improvement:  0.0130209682878
iteration 36 	loss: 4276.64611798 	improvement:  0.0108743795296
iteration 37 	loss: 4237.75571513 	improvement:  0.0090936686777
iteration 38 	loss: 4205.46001702 	improvement:  0.00762094379379
iteration 39 	loss: 4178.51244812 	improvement:  0.00640775772079
iteration 40 	loss: 4156.00233024 	improvement:  0.00538711279834
iteration 41 	loss: 4137.13915543 	improvement:  0.00453877868802
iteration 42 	loss: 4121.30354684 	improvement:  0.0038276712476
iteration 43 	loss: 4107.96094746 	improvement:  0.00323747067616
iteration 44 	loss: 4096.66331995 	improvement:  0.00275017889723
iteration 45 	loss: 4087.08891034 	improvement:  0.00233712386554
iteration 46 	loss: 4078.94474433 	improvement:  0.00199265692236
iteration 47 	loss: 4071.98456501 	improvement:  0.00170636763981
iteration 48 	loss: 4066.01255055 	improvement:  0.00146661028887
iteration 49 	loss: 4060.88182085 	improvement:  0.00126185781298
iteration 50 	loss: 4056.45416609 	improvement:  0.0010903185457
iteration 51 	loss: 4052.62850506 	improvement:  0.000943104708882
iteration 52 	loss: 4049.31284669 	improvement:  0.000818150088181
iteration 53 	loss: 4046.43342279 	improvement:  0.000711089514082
iteration 54 	loss: 4043.9207252 	improvement:  0.000620966002354
iteration 55 	loss: 4041.72226488 	improvement:  0.000543645752581
iteration 56 	loss: 4039.79469886 	improvement:  0.000476916988926
iteration 57 	loss: 4038.10254772 	improvement:  0.000418870579524
iteration 58 	loss: 4036.61296156 	improvement:  0.000368882696952
iteration 59 	loss: 4035.30012365 	improvement:  0.000325232545895
iteration 60 	loss: 4034.14089812 	improvement:  0.000287271205878
iteration 61 	loss: 4033.11567456 	improvement:  0.000254136774818
iteration 62 	loss: 4032.20743132 	improvement:  0.000225196426051
iteration 63 	loss: 4031.40158071 	improvement:  0.000199853461744
iteration 64 	loss: 4030.68572394 	improvement:  0.000177570197004
iteration 65 	loss: 4030.04901093 	improvement:  0.000157966424102
iteration 66 	loss: 4029.48206666 	improvement:  0.000140679247399
iteration 67 	loss: 4028.97693414 	improvement:  0.000125359168815
iteration 68 	loss: 4028.5263164 	improvement:  0.000111844210202
iteration 69 	loss: 4028.12391209 	improvement:  9.98887139478e-05
iteration 70 	loss: 4027.76434655 	improvement:  8.92637723758e-05
iteration 71 	loss: 4027.4428756 	improvement:  7.98137433973e-05
iteration 72 	loss: 4027.15525451 	improvement:  7.14153120861e-05
iteration 73 	loss: 4026.89776295 	improvement:  6.39388213905e-05
iteration 74 	loss: 4026.66717588 	improvement:  5.72617153958e-05
iteration 75 	loss: 4026.46055775 	improvement:  5.13124411518e-05
iteration 76 	loss: 4026.27538503 	improvement:  4.59889573768e-05
iteration 77 	loss: 4026.10935579 	improvement:  4.12364337937e-05
iteration 78 	loss: 4025.96040041 	improvement:  3.69973519611e-05
iteration 79 	loss: 4025.82673438 	improvement:  3.32010279352e-05
iteration 80 	loss: 4025.70675796 	improvement:  2.98016861444e-05
iteration 81 	loss: 4025.59903476 	improvement:  2.67588299712e-05
iteration 82 	loss: 4025.50228634 	improvement:  2.40332960101e-05
iteration 83 	loss: 4025.41537451 	improvement:  2.15903067686e-05
iteration 84 	loss: 4025.33728504 	improvement:  1.93991095414e-05
iteration 85 	loss: 4025.26711782 	improvement:  1.74313880481e-05
iteration 86 	loss: 4025.20405207 	improvement:  1.56674714582e-05
iteration 87 	loss: 4025.14737629 	improvement:  1.40802239754e-05
iteration 88 	loss: 4025.09642923 	improvement:  1.26571913442e-05
iteration 89 	loss: 4025.05062378 	improvement:  1.13799633662e-05
iteration 90 	loss: 4025.00943648 	improvement:  1.02327412248e-05
iteration 91 	loss: 4024.97239792 	improvement:  9.20210387441e-06
iteration 92 	loss: 4024.93908786 	improvement:  8.27584881756e-06
iteration 93 	loss: 4024.90912906 	improvement:  7.44329404641e-06
iteration 94 	loss: 4024.88218274 	improvement:  6.69488844991e-06
iteration 95 	loss: 4024.85794482 	improvement:  6.02201900565e-06
iteration 96 	loss: 4024.83614108 	improvement:  5.4172698165e-06
iteration 97 	loss: 4024.81652614 	improvement:  4.8734747161e-06
iteration 98 	loss: 4024.79887959 	improvement:  4.38443577876e-06
iteration 99 	loss: 4024.78300328 	improvement:  3.94462292023e-06
iteration 100 	loss: 4024.76871895 	improvement:  3.54909382905e-06
--- Execution Time ---
--- 2.39756608009 seconds ---




### Sparse-svm

python bismarck_front.py sparse-svm-spec.py
A shuffled table __bismarck_shuffled_dblife_122 is created for training
iteration 1 	loss: 41413.0027284 	improvement:  None
iteration 2 	loss: 38760.7693803 	improvement:  0.0640434929466
iteration 3 	loss: 35890.6570892 	improvement:  0.0740468349056
iteration 4 	loss: 32931.1736137 	improvement:  0.0824583252453
iteration 5 	loss: 32207.2368944 	improvement:  0.0219833258231
iteration 6 	loss: 30320.0005813 	improvement:  0.0585966538894
iteration 7 	loss: 29782.9388364 	improvement:  0.0177131178976
iteration 8 	loss: 28050.1308736 	improvement:  0.0581812282629
iteration 9 	loss: 27450.3702318 	improvement:  0.0213817413007
iteration 10 	loss: 25817.3362403 	improvement:  0.0594904177139
iteration 11 	loss: 25456.7349021 	improvement:  0.0139674106929
iteration 12 	loss: 24605.0281727 	improvement:  0.0334570294511
iteration 13 	loss: 24083.3562531 	improvement:  0.0212018419944
iteration 14 	loss: 23595.2908961 	improvement:  0.0202656702767
iteration 15 	loss: 22490.6535404 	improvement:  0.046816009205
iteration 16 	loss: 21754.4673009 	improvement:  0.0327329856456
iteration 17 	loss: 20845.6238762 	improvement:  0.0417773238041
iteration 18 	loss: 20074.4744415 	improvement:  0.036993348788
iteration 19 	loss: 19106.9902481 	improvement:  0.0481947458298
iteration 20 	loss: 18385.0939632 	improvement:  0.0377817895744
iteration 21 	loss: 17315.2904041 	improvement:  0.0581886370143
iteration 22 	loss: 16549.6130564 	improvement:  0.0442197231357
iteration 23 	loss: 15323.9605249 	improvement:  0.0740592862981
iteration 24 	loss: 14420.2155705 	improvement:  0.0589759385613
iteration 25 	loss: 13329.590565 	improvement:  0.0756316713936
iteration 26 	loss: 12273.1804715 	improvement:  0.0792530039357
iteration 27 	loss: 11182.2883119 	improvement:  0.0888842270456
iteration 28 	loss: 10158.8114587 	improvement:  0.0915266021253
iteration 29 	loss: 9237.48306662 	improvement:  0.0906925377898
iteration 30 	loss: 8346.33065187 	improvement:  0.0964713448811
iteration 31 	loss: 7475.11691271 	improvement:  0.104382845049
iteration 32 	loss: 6782.07952317 	improvement:  0.0927125819742
iteration 33 	loss: 6162.54405615 	improvement:  0.0913488945247
iteration 34 	loss: 5667.65575034 	improvement:  0.0803058446816
iteration 35 	loss: 5262.45299271 	improvement:  0.0714938901497
iteration 36 	loss: 4913.30686741 	improvement:  0.0663466497065
iteration 37 	loss: 4641.11128926 	improvement:  0.0553996698139
iteration 38 	loss: 4434.4277357 	improvement:  0.0445332035111
iteration 39 	loss: 4245.0836146 	improvement:  0.042698659756
iteration 40 	loss: 4095.40910396 	improvement:  0.0352583186161
iteration 41 	loss: 3975.99305415 	improvement:  0.0291585154954
iteration 42 	loss: 3875.59865916 	improvement:  0.0252501434523
iteration 43 	loss: 3792.19557326 	improvement:  0.0215200523173
iteration 44 	loss: 3715.88140644 	improvement:  0.0201240060913
iteration 45 	loss: 3659.38167093 	improvement:  0.0152049350704
iteration 46 	loss: 3606.35569379 	improvement:  0.0144904199425
iteration 47 	loss: 3566.31635377 	improvement:  0.0111024378669
iteration 48 	loss: 3532.20368331 	improvement:  0.00956523961404
iteration 49 	loss: 3501.68475662 	improvement:  0.0086401944565
iteration 50 	loss: 3475.44754227 	improvement:  0.0074927402605
iteration 51 	loss: 3454.57003326 	improvement:  0.00600714260635
iteration 52 	loss: 3435.1441124 	improvement:  0.00562325287141
iteration 53 	loss: 3419.14344484 	improvement:  0.0046579319639
iteration 54 	loss: 3404.35124917 	improvement:  0.00432628695124
iteration 55 	loss: 3393.28931148 	improvement:  0.00324935263065
iteration 56 	loss: 3382.402504 	improvement:  0.00320833459293
iteration 57 	loss: 3373.0120195 	improvement:  0.0027762764748
iteration 58 	loss: 3364.94989886 	improvement:  0.00239018437767
iteration 59 	loss: 3357.71796855 	improvement:  0.00214919405299
iteration 60 	loss: 3351.83199738 	improvement:  0.00175296770836
iteration 61 	loss: 3346.20830135 	improvement:  0.00167779770315
iteration 62 	loss: 3341.30612329 	improvement:  0.00146499489117
iteration 63 	loss: 3337.14076822 	improvement:  0.0012466247982
iteration 64 	loss: 3333.25643791 	improvement:  0.00116396957025
iteration 65 	loss: 3330.17794953 	improvement:  0.000923567819795
iteration 66 	loss: 3327.10261871 	improvement:  0.000923473419934
iteration 67 	loss: 3324.38598399 	improvement:  0.00081651665992
iteration 68 	loss: 3322.12515846 	improvement:  0.000680073114349
iteration 69 	loss: 3320.20848716 	improvement:  0.000576941328336
iteration 70 	loss: 3318.46948951 	improvement:  0.000523761582318
iteration 71 	loss: 3316.78932507 	improvement:  0.000506307033436
iteration 72 	loss: 3315.43221607 	improvement:  0.000409163459633
iteration 73 	loss: 3314.10489963 	improvement:  0.000400344919225
iteration 74 	loss: 3312.96566396 	improvement:  0.000343753653591
iteration 75 	loss: 3311.97514451 	improvement:  0.000298982710743
iteration 76 	loss: 3311.05225085 	improvement:  0.000278653557081
iteration 77 	loss: 3310.27141966 	improvement:  0.00023582569176
iteration 78 	loss: 3309.50666476 	improvement:  0.000231024834736
iteration 79 	loss: 3308.89785932 	improvement:  0.00018395655136
iteration 80 	loss: 3308.31518825 	improvement:  0.000176092189965
iteration 81 	loss: 3307.81887098 	improvement:  0.000150021157407
iteration 82 	loss: 3307.34660016 	improvement:  0.000142774087839
iteration 83 	loss: 3306.92536473 	improvement:  0.000127363557678
iteration 84 	loss: 3306.56613113 	improvement:  0.000108630694917
iteration 85 	loss: 3306.22287704 	improvement:  0.000103809835049
iteration 86 	loss: 3305.93657303 	improvement:  8.65954963829e-05
iteration 87 	loss: 3305.65276783 	improvement:  8.58471421337e-05
iteration 88 	loss: 3305.42972454 	improvement:  6.74732941769e-05
iteration 89 	loss: 3305.20974433 	improvement:  6.6551169083e-05
iteration 90 	loss: 3305.02526639 	improvement:  5.58142915155e-05
iteration 91 	loss: 3304.84736445 	improvement:  5.38277110282e-05
iteration 92 	loss: 3304.68755749 	improvement:  4.83553221608e-05
iteration 93 	loss: 3304.55693264 	improvement:  3.95271393581e-05
iteration 94 	loss: 3304.42486578 	improvement:  3.9965072856e-05
iteration 95 	loss: 3304.31142591 	improvement:  3.4329688514e-05
iteration 96 	loss: 3304.21257612 	improvement:  2.99153954239e-05
iteration 97 	loss: 3304.12250577 	improvement:  2.72592478525e-05
iteration 98 	loss: 3304.04085068 	improvement:  2.4713096596e-05
iteration 99 	loss: 3303.9668346 	improvement:  2.24016821054e-05
iteration 100 	loss: 3303.90183047 	improvement:  1.96745709488e-05
--- Execution Time ---
--- 1.91258907318 seconds ---