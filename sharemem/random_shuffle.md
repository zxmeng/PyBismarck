### dense-logit

python bismarck_front.py dense-logit-spec.py

A shuffled table __bismarck_shuffled_forest_1 is created for training
iteration 1 	loss: 347996.559458 	improvement:  None
iteration 2 	loss: 347646.188999 	improvement:  0.00100682161825
iteration 3 	loss: 347634.750269 	improvement:  3.29033681857e-05
iteration 4 	loss: 347634.795751 	improvement:  -1.30832406677e-07
iteration 5 	loss: 347634.896794 	improvement:  -2.90660432211e-07
iteration 6 	loss: 347634.923156 	improvement:  -7.58316964748e-08
iteration 7 	loss: 347634.927948 	improvement:  -1.37858215042e-08
iteration 8 	loss: 347634.928805 	improvement:  -2.46407922349e-09
iteration 9 	loss: 347634.928849 	improvement:  -1.25985659234e-10
iteration 10 	loss: 347634.928884 	improvement:  -1.01675686417e-10
iteration 11 	loss: 347634.928891 	improvement:  -2.00670673683e-11
iteration 12 	loss: 347634.928893 	improvement:  -4.00129088664e-12
iteration 13 	loss: 347634.928893 	improvement:  -7.70889368624e-13
iteration 14 	loss: 347634.928893 	improvement:  -1.35123310269e-13
iteration 15 	loss: 347634.928893 	improvement:  -2.59530521583e-14
iteration 16 	loss: 347634.928893 	improvement:  -5.6929275702e-15
iteration 17 	loss: 347634.928893 	improvement:  0.0
iteration 18 	loss: 347634.928893 	improvement:  0.0
iteration 19 	loss: 347634.928893 	improvement:  -3.01390283128e-15
iteration 20 	loss: 347634.928893 	improvement:  0.0
iteration 21 	loss: 347634.928893 	improvement:  0.0
iteration 22 	loss: 347634.928893 	improvement:  0.0
iteration 23 	loss: 347634.928893 	improvement:  0.0
iteration 24 	loss: 347634.928893 	improvement:  0.0
iteration 25 	loss: 347634.928893 	improvement:  0.0
iteration 26 	loss: 347634.928893 	improvement:  0.0
iteration 27 	loss: 347634.928893 	improvement:  0.0
iteration 28 	loss: 347634.928893 	improvement:  0.0
iteration 29 	loss: 347634.928893 	improvement:  0.0
iteration 30 	loss: 347634.928893 	improvement:  0.0
iteration 31 	loss: 347634.928893 	improvement:  0.0
iteration 32 	loss: 347634.928893 	improvement:  0.0
iteration 33 	loss: 347634.928893 	improvement:  0.0
iteration 34 	loss: 347634.928893 	improvement:  0.0
iteration 35 	loss: 347634.928893 	improvement:  0.0
iteration 36 	loss: 347634.928893 	improvement:  0.0
iteration 37 	loss: 347634.928893 	improvement:  0.0
iteration 38 	loss: 347634.928893 	improvement:  0.0
iteration 39 	loss: 347634.928893 	improvement:  0.0
iteration 40 	loss: 347634.928893 	improvement:  0.0
iteration 41 	loss: 347634.928893 	improvement:  0.0
iteration 42 	loss: 347634.928893 	improvement:  0.0
iteration 43 	loss: 347634.928893 	improvement:  0.0
iteration 44 	loss: 347634.928893 	improvement:  0.0
iteration 45 	loss: 347634.928893 	improvement:  0.0
iteration 46 	loss: 347634.928893 	improvement:  0.0
iteration 47 	loss: 347634.928893 	improvement:  0.0
iteration 48 	loss: 347634.928893 	improvement:  0.0
iteration 49 	loss: 347634.928893 	improvement:  0.0
iteration 50 	loss: 347634.928893 	improvement:  0.0
iteration 51 	loss: 347634.928893 	improvement:  0.0
iteration 52 	loss: 347634.928893 	improvement:  0.0
iteration 53 	loss: 347634.928893 	improvement:  0.0
iteration 54 	loss: 347634.928893 	improvement:  0.0
iteration 55 	loss: 347634.928893 	improvement:  0.0
iteration 56 	loss: 347634.928893 	improvement:  0.0
iteration 57 	loss: 347634.928893 	improvement:  0.0
iteration 58 	loss: 347634.928893 	improvement:  0.0
iteration 59 	loss: 347634.928893 	improvement:  0.0
iteration 60 	loss: 347634.928893 	improvement:  0.0
iteration 61 	loss: 347634.928893 	improvement:  0.0
iteration 62 	loss: 347634.928893 	improvement:  0.0
iteration 63 	loss: 347634.928893 	improvement:  0.0
iteration 64 	loss: 347634.928893 	improvement:  0.0
iteration 65 	loss: 347634.928893 	improvement:  0.0
iteration 66 	loss: 347634.928893 	improvement:  0.0
iteration 67 	loss: 347634.928893 	improvement:  0.0
iteration 68 	loss: 347634.928893 	improvement:  0.0
iteration 69 	loss: 347634.928893 	improvement:  0.0
iteration 70 	loss: 347634.928893 	improvement:  0.0
iteration 71 	loss: 347634.928893 	improvement:  0.0
iteration 72 	loss: 347634.928893 	improvement:  0.0
iteration 73 	loss: 347634.928893 	improvement:  0.0
iteration 74 	loss: 347634.928893 	improvement:  0.0
iteration 75 	loss: 347634.928893 	improvement:  0.0
iteration 76 	loss: 347634.928893 	improvement:  0.0
iteration 77 	loss: 347634.928893 	improvement:  0.0
iteration 78 	loss: 347634.928893 	improvement:  0.0
iteration 79 	loss: 347634.928893 	improvement:  0.0
iteration 80 	loss: 347634.928893 	improvement:  0.0
iteration 81 	loss: 347634.928893 	improvement:  0.0
iteration 82 	loss: 347634.928893 	improvement:  0.0
iteration 83 	loss: 347634.928893 	improvement:  0.0
iteration 84 	loss: 347634.928893 	improvement:  0.0
iteration 85 	loss: 347634.928893 	improvement:  0.0
iteration 86 	loss: 347634.928893 	improvement:  0.0
iteration 87 	loss: 347634.928893 	improvement:  0.0
iteration 88 	loss: 347634.928893 	improvement:  0.0
iteration 89 	loss: 347634.928893 	improvement:  0.0
iteration 90 	loss: 347634.928893 	improvement:  0.0
iteration 91 	loss: 347634.928893 	improvement:  0.0
iteration 92 	loss: 347634.928893 	improvement:  0.0
iteration 93 	loss: 347634.928893 	improvement:  0.0
iteration 94 	loss: 347634.928893 	improvement:  0.0
iteration 95 	loss: 347634.928893 	improvement:  0.0
iteration 96 	loss: 347634.928893 	improvement:  0.0
iteration 97 	loss: 347634.928893 	improvement:  0.0
iteration 98 	loss: 347634.928893 	improvement:  0.0
iteration 99 	loss: 347634.928893 	improvement:  0.0
iteration 100 	loss: 347634.928893 	improvement:  0.0
--- Execution Time ---
--- 115.428755045 seconds ---


### sparse-logit

python bismarck_front.py sparse-logit-spec.py
A shuffled table __bismarck_shuffled_dblife_22 is created for training
iteration 1 	loss: 5413.25529434 	improvement:  None
iteration 2 	loss: 4722.81231009 	improvement:  0.127546724976
iteration 3 	loss: 4398.68360736 	improvement:  0.0686304433574
iteration 4 	loss: 4190.18970321 	improvement:  0.0473991591027
iteration 5 	loss: 4043.47833892 	improvement:  0.0350130601908
iteration 6 	loss: 3934.31275888 	improvement:  0.0269979386293
iteration 7 	loss: 3850.4968843 	improvement:  0.0213038158657
iteration 8 	loss: 3784.64934388 	improvement:  0.0171010501759
iteration 9 	loss: 3731.69114349 	improvement:  0.0139928948699
iteration 10 	loss: 3688.71905113 	improvement:  0.0115154472083
iteration 11 	loss: 3653.6173771 	improvement:  0.00951595216346
iteration 12 	loss: 3624.54627494 	improvement:  0.00795679983828
iteration 13 	loss: 3600.27907465 	improvement:  0.00669523809253
iteration 14 	loss: 3579.99255204 	improvement:  0.00563470836364
iteration 15 	loss: 3562.95606663 	improvement:  0.00475880470858
iteration 16 	loss: 3548.59936594 	improvement:  0.00402943522594
iteration 17 	loss: 3536.47748554 	improvement:  0.00341596194963
iteration 18 	loss: 3526.21537369 	improvement:  0.00290178910678
iteration 19 	loss: 3517.45134612 	improvement:  0.00248539202688
iteration 20 	loss: 3509.97313967 	improvement:  0.00212602981941
iteration 21 	loss: 3503.53776007 	improvement:  0.00183345551214
iteration 22 	loss: 3497.98675513 	improvement:  0.00158439991726
iteration 23 	loss: 3493.18207541 	improvement:  0.00137355572001
iteration 24 	loss: 3489.01405613 	improvement:  0.00119318695567
iteration 25 	loss: 3485.36786042 	improvement:  0.00104505044802
iteration 26 	loss: 3482.17713477 	improvement:  0.000915463097465
iteration 27 	loss: 3479.36850799 	improvement:  0.00080657205748
iteration 28 	loss: 3476.90188269 	improvement:  0.000708929018897
iteration 29 	loss: 3474.7230006 	improvement:  0.000626673446049
iteration 30 	loss: 3472.80128084 	improvement:  0.000553056967311
iteration 31 	loss: 3471.10415618 	improvement:  0.000488690402832
iteration 32 	loss: 3469.60190552 	improvement:  0.000432787549989
iteration 33 	loss: 3468.26760256 	improvement:  0.000384569469047
iteration 34 	loss: 3467.0833774 	improvement:  0.000341445728132
iteration 35 	loss: 3466.03101952 	improvement:  0.000303528288333
iteration 36 	loss: 3465.09290706 	improvement:  0.00027065898961
iteration 37 	loss: 3464.25429808 	improvement:  0.000242016305765
iteration 38 	loss: 3463.50519692 	improvement:  0.000216237347751
iteration 39 	loss: 3462.83673634 	improvement:  0.000193001175363
iteration 40 	loss: 3462.23814416 	improvement:  0.000172861798666
iteration 41 	loss: 3461.70209635 	improvement:  0.000154826960481
iteration 42 	loss: 3461.22343275 	improvement:  0.000138274058852
iteration 43 	loss: 3460.79430433 	improvement:  0.000123981715378
iteration 44 	loss: 3460.40951454 	improvement:  0.000111185397571
iteration 45 	loss: 3460.06440359 	improvement:  9.97312446085e-05
iteration 46 	loss: 3459.75454991 	improvement:  8.95514192362e-05
iteration 47 	loss: 3459.47633184 	improvement:  8.04155519405e-05
iteration 48 	loss: 3459.22658022 	improvement:  7.2193477198e-05
iteration 49 	loss: 3459.00226835 	improvement:  6.48445133062e-05
iteration 50 	loss: 3458.80073073 	improvement:  5.82646702559e-05
iteration 51 	loss: 3458.6196627 	improvement:  5.23499429272e-05
iteration 52 	loss: 3458.45702061 	improvement:  4.70251461454e-05
iteration 53 	loss: 3458.3108399 	improvement:  4.22676083782e-05
iteration 54 	loss: 3458.17942183 	improvement:  3.80006543002e-05
iteration 55 	loss: 3458.06129006 	improvement:  3.41601037542e-05
iteration 56 	loss: 3457.95508763 	improvement:  3.07115518528e-05
iteration 57 	loss: 3457.85959896 	improvement:  2.7614203282e-05
iteration 58 	loss: 3457.77373906 	improvement:  2.48303592275e-05
iteration 59 	loss: 3457.69664594 	improvement:  2.22955932336e-05
iteration 60 	loss: 3457.62730933 	improvement:  2.00528331169e-05
iteration 61 	loss: 3457.56503728 	improvement:  1.80100538719e-05
iteration 62 	loss: 3457.50908131 	improvement:  1.61836324166e-05
iteration 63 	loss: 3457.45878761 	improvement:  1.45462236881e-05
iteration 64 	loss: 3457.41359207 	improvement:  1.30718952173e-05
iteration 65 	loss: 3457.37295169 	improvement:  1.17545622581e-05
iteration 66 	loss: 3457.33638676 	improvement:  1.05759297249e-05
iteration 67 	loss: 3457.30349063 	improvement:  9.51487649163e-06
iteration 68 	loss: 3457.27388878 	improvement:  8.56212195155e-06
iteration 69 	loss: 3457.24725225 	improvement:  7.70448918338e-06
iteration 70 	loss: 3457.22328243 	improvement:  6.93320729499e-06
iteration 71 	loss: 3457.20171468 	improvement:  6.23846171278e-06
iteration 72 	loss: 3457.1823068 	improvement:  5.61375414902e-06
iteration 73 	loss: 3457.1648413 	improvement:  5.05194513345e-06
iteration 74 	loss: 3457.14912456 	improvement:  4.54613676561e-06
iteration 75 	loss: 3457.13498353 	improvement:  4.09037406844e-06
iteration 76 	loss: 3457.12226047 	improvement:  3.68023259741e-06
iteration 77 	loss: 3457.11081068 	improvement:  3.3119405672e-06
iteration 78 	loss: 3457.10050676 	improvement:  2.9804993691e-06
iteration 79 	loss: 3457.09123442 	improvement:  2.68211719964e-06
iteration 80 	loss: 3457.08289068 	improvement:  2.41351363735e-06
iteration 81 	loss: 3457.0753819 	improvement:  2.17199756193e-06
iteration 82 	loss: 3457.06862433 	improvement:  1.95470833102e-06
iteration 83 	loss: 3457.06254277 	improvement:  1.75916530766e-06
iteration 84 	loss: 3457.05706956 	improvement:  1.58319743489e-06
iteration 85 	loss: 3457.05214382 	improvement:  1.4248381675e-06
iteration 86 	loss: 3457.04771077 	improvement:  1.28231981913e-06
iteration 87 	loss: 3457.04372112 	improvement:  1.15406215766e-06
iteration 88 	loss: 3457.04013117 	improvement:  1.03844467406e-06
iteration 89 	loss: 3457.03690111 	improvement:  9.34344441228e-07
iteration 90 	loss: 3457.03399407 	improvement:  8.40905114191e-07
iteration 91 	loss: 3457.03137774 	improvement:  7.56811181052e-07
iteration 92 	loss: 3457.02902307 	improvement:  6.81127283131e-07
iteration 93 	loss: 3457.02690386 	improvement:  6.1301231084e-07
iteration 94 	loss: 3457.02499659 	improvement:  5.5170922964e-07
iteration 95 	loss: 3457.02328005 	improvement:  4.96536823352e-07
iteration 96 	loss: 3457.02173517 	improvement:  4.46881934235e-07
iteration 97 	loss: 3457.02034486 	improvement:  4.02170450835e-07
iteration 98 	loss: 3457.01909367 	improvement:  3.61927786199e-07
iteration 99 	loss: 3457.0179676 	improvement:  3.25733951448e-07
iteration 100 	loss: 3457.0169542 	improvement:  2.93140900525e-07
--- Execution Time ---
--- 2.46454620361 seconds ---


### dense-svm

python bismarck_front.py dense-svm-spec.py
A shuffled table __bismarck_shuffled_forest_12 is created for training
iteration 1 	loss: 380820.476635 	improvement:  None
iteration 2 	loss: 380811.28073 	improvement:  2.41476130623e-05
iteration 3 	loss: 380848.510695 	improvement:  -9.77648688312e-05
iteration 4 	loss: 380821.628172 	improvement:  7.05858680711e-05
iteration 5 	loss: 380847.859786 	improvement:  -6.88816294204e-05
iteration 6 	loss: 380858.987116 	improvement:  -2.92172564374e-05
iteration 7 	loss: 380836.57835 	improvement:  5.88374349669e-05
iteration 8 	loss: 380872.945091 	improvement:  -9.54917236169e-05
iteration 9 	loss: 380866.283211 	improvement:  1.74910835433e-05
iteration 10 	loss: 380828.517571 	improvement:  9.91572142057e-05
iteration 11 	loss: 380805.654609 	improvement:  6.00347957102e-05
iteration 12 	loss: 380805.709618 	improvement:  -1.44455391661e-07
iteration 13 	loss: 380823.22473 	improvement:  -4.59948781771e-05
iteration 14 	loss: 380849.213054 	improvement:  -6.82424867033e-05
iteration 15 	loss: 380818.880651 	improvement:  7.96441273484e-05
iteration 16 	loss: 380867.156286 	improvement:  -0.000126767966463
iteration 17 	loss: 380831.211357 	improvement:  9.43765515423e-05
iteration 18 	loss: 380889.612579 	improvement:  -0.000153351983344
iteration 19 	loss: 380860.240263 	improvement:  7.71150346635e-05
iteration 20 	loss: 380838.287817 	improvement:  5.76391123022e-05
iteration 21 	loss: 380836.067059 	improvement:  5.83123537746e-06
iteration 22 	loss: 380797.39077 	improvement:  0.000101556267319
iteration 23 	loss: 380853.05726 	improvement:  -0.000146184012368
iteration 24 	loss: 380847.168015 	improvement:  1.54633016875e-05
iteration 25 	loss: 380891.518067 	improvement:  -0.000116451050261
iteration 26 	loss: 380886.722894 	improvement:  1.25893419952e-05
iteration 27 	loss: 380853.967203 	improvement:  8.59985100009e-05
iteration 28 	loss: 380843.273146 	improvement:  2.80791540248e-05
iteration 29 	loss: 380873.964478 	improvement:  -8.05878265894e-05
iteration 30 	loss: 380857.440722 	improvement:  4.33837892744e-05
iteration 31 	loss: 380852.509471 	improvement:  1.29477586487e-05
iteration 32 	loss: 380821.415046 	improvement:  8.1644270298e-05
iteration 33 	loss: 380822.328093 	improvement:  -2.39757159631e-06
iteration 34 	loss: 380873.040351 	improvement:  -0.000133165137535
iteration 35 	loss: 380862.89302 	improvement:  2.66422904591e-05
iteration 36 	loss: 380860.615595 	improvement:  5.97964636811e-06
iteration 37 	loss: 380843.242624 	improvement:  4.56150367841e-05
iteration 38 	loss: 380831.640878 	improvement:  3.04633097416e-05
iteration 39 	loss: 380865.235872 	improvement:  -8.8214818694e-05
iteration 40 	loss: 380850.073857 	improvement:  3.98093961983e-05
iteration 41 	loss: 380845.989447 	improvement:  1.07244569251e-05
iteration 42 	loss: 380843.233712 	improvement:  7.23582689679e-06
iteration 43 	loss: 380784.493012 	improvement:  0.000154238527839
iteration 44 	loss: 380834.38979 	improvement:  -0.000131036791087
iteration 45 	loss: 380842.429655 	improvement:  -2.11111845111e-05
iteration 46 	loss: 380823.010794 	improvement:  5.09892277958e-05
iteration 47 	loss: 380847.752912 	improvement:  -6.49701232638e-05
iteration 48 	loss: 380845.160565 	improvement:  6.80677850445e-06
iteration 49 	loss: 380829.665596 	improvement:  4.06857445109e-05
iteration 50 	loss: 380861.820956 	improvement:  -8.44350192037e-05
iteration 51 	loss: 380848.250544 	improvement:  3.56308023233e-05
iteration 52 	loss: 380795.604369 	improvement:  0.000138233994518
iteration 53 	loss: 380847.341919 	improvement:  -0.000135866983139
iteration 54 	loss: 380879.324386 	improvement:  -8.39771306419e-05
iteration 55 	loss: 380874.152054 	improvement:  1.35799774149e-05
iteration 56 	loss: 380910.93259 	improvement:  -9.65687397601e-05
iteration 57 	loss: 380877.520291 	improvement:  8.77168307636e-05
iteration 58 	loss: 380862.027837 	improvement:  4.06756828735e-05
iteration 59 	loss: 380871.585367 	improvement:  -2.50944660203e-05
iteration 60 	loss: 380870.389239 	improvement:  3.14050016853e-06
iteration 61 	loss: 380862.561013 	improvement:  2.05535180081e-05
iteration 62 	loss: 380855.722063 	improvement:  1.79564762149e-05
iteration 63 	loss: 380858.487786 	improvement:  -7.26186493141e-06
iteration 64 	loss: 380827.109985 	improvement:  8.23870339333e-05
iteration 65 	loss: 380890.098169 	improvement:  -0.00016539837111
iteration 66 	loss: 380849.068784 	improvement:  0.000107719747258
iteration 67 	loss: 380835.156033 	improvement:  3.65308761827e-05
iteration 68 	loss: 380836.581094 	improvement:  -3.74193417919e-06
iteration 69 	loss: 380829.179344 	improvement:  1.94355008301e-05
iteration 70 	loss: 380830.242103 	improvement:  -2.79064553244e-06
iteration 71 	loss: 380837.457299 	improvement:  -1.89459632596e-05
iteration 72 	loss: 380902.522944 	improvement:  -0.000170848858302
iteration 73 	loss: 380867.03982 	improvement:  9.31553906516e-05
iteration 74 	loss: 380798.016737 	improvement:  0.000181226191913
iteration 75 	loss: 380835.826812 	improvement:  -9.92916799146e-05
iteration 76 	loss: 380859.741959 	improvement:  -6.27964733234e-05
iteration 77 	loss: 380857.199326 	improvement:  6.6760336939e-06
iteration 78 	loss: 380876.743129 	improvement:  -5.13153046458e-05
iteration 79 	loss: 380826.631159 	improvement:  0.00013157004668
iteration 80 	loss: 380833.281501 	improvement:  -1.74629145176e-05
iteration 81 	loss: 380821.565579 	improvement:  3.07639132478e-05
iteration 82 	loss: 380841.990484 	improvement:  -5.36337931807e-05
iteration 83 	loss: 380846.236963 	improvement:  -1.11502380911e-05
iteration 84 	loss: 380859.061956 	improvement:  -3.36749891196e-05
iteration 85 	loss: 380806.293659 	improvement:  0.000138550719171
iteration 86 	loss: 380876.127884 	improvement:  -0.000183385163501
iteration 87 	loss: 380877.453014 	improvement:  -3.47916362832e-06
iteration 88 	loss: 380867.609909 	improvement:  2.58432342636e-05
iteration 89 	loss: 380850.351439 	improvement:  4.5313567342e-05
iteration 90 	loss: 380850.077141 	improvement:  7.20223807018e-07
iteration 91 	loss: 380836.728364 	improvement:  3.50499538223e-05
iteration 92 	loss: 380859.917366 	improvement:  -6.08896158248e-05
iteration 93 	loss: 380837.738431 	improvement:  5.82338390592e-05
iteration 94 	loss: 380858.159913 	improvement:  -5.36225277939e-05
iteration 95 	loss: 380824.15104 	improvement:  8.92953754063e-05
iteration 96 	loss: 380880.067785 	improvement:  -0.000146830878465
iteration 97 	loss: 380811.787011 	improvement:  0.000179271061883
iteration 98 	loss: 380826.674014 	improvement:  -3.90928108577e-05
iteration 99 	loss: 380839.848473 	improvement:  -3.45943694965e-05
iteration 100 	loss: 380852.415301 	improvement:  -3.29976729284e-05
--- Execution Time ---
--- 100.882956982 seconds ---







### sparse-svm

python bismarck_front.py sparse-svm-spec.py
A shuffled table __bismarck_shuffled_dblife_122 is created for training
iteration 1 	loss: 6705.56071191 	improvement:  None
iteration 2 	loss: 5515.90507609 	improvement:  0.177413297252
iteration 3 	loss: 4879.96148513 	improvement:  0.115292700324
iteration 4 	loss: 4496.19561271 	improvement:  0.078641168295
iteration 5 	loss: 4208.65676019 	improvement:  0.0639515886944
iteration 6 	loss: 4009.31194595 	improvement:  0.0473654245526
iteration 7 	loss: 3744.91142226 	improvement:  0.0659466081116
iteration 8 	loss: 3582.34643214 	improvement:  0.0434095688223
iteration 9 	loss: 3529.00829049 	improvement:  0.0148891634732
iteration 10 	loss: 3409.5458234 	improvement:  0.0338515688426
iteration 11 	loss: 3349.33491935 	improvement:  0.0176595086766
iteration 12 	loss: 3248.88862894 	improvement:  0.0299899212311
iteration 13 	loss: 3174.64734999 	improvement:  0.0228512846781
iteration 14 	loss: 3121.83137099 	improvement:  0.0166368018804
iteration 15 	loss: 3067.52665898 	improvement:  0.0173951458485
iteration 16 	loss: 3010.05214764 	improvement:  0.0187364341784
iteration 17 	loss: 2990.55407721 	improvement:  0.00647765204053
iteration 18 	loss: 2948.22560843 	improvement:  0.0141540556311
iteration 19 	loss: 2917.66734118 	improvement:  0.0103649690736
iteration 20 	loss: 2897.56959188 	improvement:  0.00688829360687
iteration 21 	loss: 2858.65668996 	improvement:  0.0134294969247
iteration 22 	loss: 2851.37508379 	improvement:  0.00254721254034
iteration 23 	loss: 2825.19015768 	improvement:  0.00918326257981
iteration 24 	loss: 2806.60905177 	improvement:  0.00657693991512
iteration 25 	loss: 2796.28500395 	improvement:  0.00367847734552
iteration 26 	loss: 2783.20946333 	improvement:  0.00467604003115
iteration 27 	loss: 2770.45277197 	improvement:  0.00458344638995
iteration 28 	loss: 2760.2540061 	improvement:  0.00368126321214
iteration 29 	loss: 2750.91461386 	improvement:  0.00338352637827
iteration 30 	loss: 2741.58691521 	improvement:  0.00339076269333
iteration 31 	loss: 2736.71212313 	improvement:  0.00177809138586
iteration 32 	loss: 2728.14442467 	improvement:  0.00313065389336
iteration 33 	loss: 2722.66819234 	improvement:  0.00200731027245
iteration 34 	loss: 2717.06548241 	improvement:  0.00205780122297
iteration 35 	loss: 2713.79951482 	improvement:  0.0012020201971
iteration 36 	loss: 2707.45587868 	improvement:  0.00233754781801
iteration 37 	loss: 2705.28888501 	improvement:  0.000800380052547
iteration 38 	loss: 2702.25568913 	improvement:  0.00112120960323
iteration 39 	loss: 2699.3764907 	improvement:  0.00106547964255
iteration 40 	loss: 2695.68595697 	improvement:  0.00136718006816
iteration 41 	loss: 2693.21533896 	improvement:  0.000916508097163
iteration 42 	loss: 2691.36085992 	improvement:  0.000688574364341
iteration 43 	loss: 2689.90616369 	improvement:  0.000540505829702
iteration 44 	loss: 2688.59056039 	improvement:  0.000489088916982
iteration 45 	loss: 2686.02479844 	improvement:  0.00095431487118
iteration 46 	loss: 2684.91964496 	improvement:  0.00041144574819
iteration 47 	loss: 2683.82258799 	improvement:  0.000408599552199
iteration 48 	loss: 2682.76837551 	improvement:  0.000392802599045
iteration 49 	loss: 2681.20173971 	improvement:  0.000583962374558
iteration 50 	loss: 2680.03866611 	improvement:  0.000433788174822
iteration 51 	loss: 2679.52934693 	improvement:  0.000190041726361
iteration 52 	loss: 2679.0425127 	improvement:  0.000181686470726
iteration 53 	loss: 2678.26321951 	improvement:  0.000290884966306
iteration 54 	loss: 2677.97141297 	improvement:  0.000108953644363
iteration 55 	loss: 2677.30985712 	improvement:  0.000247036186621
iteration 56 	loss: 2676.86140269 	improvement:  0.000167501881471
iteration 57 	loss: 2676.47006516 	improvement:  0.000146192674536
iteration 58 	loss: 2676.18334899 	improvement:  0.000107124742437
iteration 59 	loss: 2675.8768382 	improvement:  0.000114532808496
iteration 60 	loss: 2675.66341571 	improvement:  7.97579642582e-05
iteration 61 	loss: 2675.39553606 	improvement:  0.000100117094862
iteration 62 	loss: 2675.16859552 	improvement:  8.48250432659e-05
iteration 63 	loss: 2675.0455689 	improvement:  4.59883599248e-05
iteration 64 	loss: 2674.80148717 	improvement:  9.12439514442e-05
iteration 65 	loss: 2674.61682855 	improvement:  6.90363846085e-05
iteration 66 	loss: 2674.53092213 	improvement:  3.21191485311e-05
iteration 67 	loss: 2674.35534233 	improvement:  6.56488237982e-05
iteration 68 	loss: 2674.28692877 	improvement:  2.55813255432e-05
iteration 69 	loss: 2674.17324976 	improvement:  4.25081570406e-05
iteration 70 	loss: 2674.092451 	improvement:  3.02144815251e-05
iteration 71 	loss: 2673.9941338 	improvement:  3.67665700426e-05
iteration 72 	loss: 2673.94570876 	improvement:  1.81096263181e-05
iteration 73 	loss: 2673.85584933 	improvement:  3.36055562556e-05
iteration 74 	loss: 2673.78269639 	improvement:  2.73585945661e-05
iteration 75 	loss: 2673.72070412 	improvement:  2.31852319612e-05
iteration 76 	loss: 2673.69213937 	improvement:  1.06835212617e-05
iteration 77 	loss: 2673.63395459 	improvement:  2.17619567239e-05
iteration 78 	loss: 2673.59666255 	improvement:  1.39480721868e-05
iteration 79 	loss: 2673.57237363 	improvement:  9.08473860338e-06
iteration 80 	loss: 2673.53589494 	improvement:  1.36441742367e-05
iteration 81 	loss: 2673.50939806 	improvement:  9.91079847858e-06
iteration 82 	loss: 2673.48337206 	improvement:  9.73477211588e-06
iteration 83 	loss: 2673.46301875 	improvement:  7.61303002764e-06
iteration 84 	loss: 2673.45205801 	improvement:  4.09982802582e-06
iteration 85 	loss: 2673.42624374 	improvement:  9.65578154006e-06
iteration 86 	loss: 2673.41381681 	improvement:  4.64831784267e-06
iteration 87 	loss: 2673.40049721 	improvement:  4.98224410172e-06
iteration 88 	loss: 2673.38250417 	improvement:  6.73039362738e-06
iteration 89 	loss: 2673.37444603 	improvement:  3.01421238729e-06
iteration 90 	loss: 2673.36146657 	improvement:  4.85508289696e-06
iteration 91 	loss: 2673.35098938 	improvement:  3.91911007207e-06
iteration 92 	loss: 2673.34524174 	improvement:  2.14997370072e-06
iteration 93 	loss: 2673.33903906 	improvement:  2.32019601255e-06
iteration 94 	loss: 2673.32987274 	improvement:  3.4287893851e-06
iteration 95 	loss: 2673.32464879 	improvement:  1.95410000586e-06
iteration 96 	loss: 2673.316601 	improvement:  3.01040266608e-06
iteration 97 	loss: 2673.31435943 	improvement:  8.3849866467e-07
iteration 98 	loss: 2673.30971313 	improvement:  1.73802912987e-06
iteration 99 	loss: 2673.30557085 	improvement:  1.5494939585e-06
iteration 100 	loss: 2673.30135569 	improvement:  1.5767612937e-06
--- Execution Time ---
--- 1.99778795242 seconds ---