### dense-logit

python bismarck_front.py dense-logit-spec.py
A shuffled table __bismarck_shuffled_forest_1 is created for training
iteration 1 	loss: 345993.361059 	improvement:  None
iteration 2 	loss: 345695.712647 	improvement:  0.000860272033781
iteration 3 	loss: 345681.259759 	improvement:  4.18081207527e-05
iteration 4 	loss: 345679.781505 	improvement:  4.27634747396e-06
iteration 5 	loss: 345679.519852 	improvement:  7.56923832523e-07
iteration 6 	loss: 345679.459883 	improvement:  1.73481749888e-07
iteration 7 	loss: 345679.445092 	improvement:  4.27885995557e-08
iteration 8 	loss: 345679.44136 	improvement:  1.0797280633e-08
iteration 9 	loss: 345679.440417 	improvement:  2.72659551663e-09
iteration 10 	loss: 345679.440179 	improvement:  6.90006395223e-10
iteration 11 	loss: 345679.440118 	improvement:  1.74789125985e-10
iteration 12 	loss: 345679.440103 	improvement:  4.43271723284e-11
iteration 13 	loss: 345679.440099 	improvement:  1.11953275044e-11
iteration 14 	loss: 345679.440098 	improvement:  2.81474437575e-12
iteration 15 	loss: 345679.440098 	improvement:  7.4056268034e-13
iteration 16 	loss: 345679.440098 	improvement:  1.87919043033e-13
iteration 17 	loss: 345679.440098 	improvement:  2.32373010202e-14
iteration 18 	loss: 345679.440098 	improvement:  2.86256606771e-15
iteration 19 	loss: 345679.440098 	improvement:  5.72513213542e-15
iteration 20 	loss: 345679.440098 	improvement:  3.03095230699e-15
iteration 21 	loss: 345679.440098 	improvement:  0.0
iteration 22 	loss: 345679.440098 	improvement:  0.0
iteration 23 	loss: 345679.440098 	improvement:  0.0
iteration 24 	loss: 345679.440098 	improvement:  0.0
iteration 25 	loss: 345679.440098 	improvement:  0.0
iteration 26 	loss: 345679.440098 	improvement:  0.0
iteration 27 	loss: 345679.440098 	improvement:  0.0
iteration 28 	loss: 345679.440098 	improvement:  0.0
iteration 29 	loss: 345679.440098 	improvement:  0.0
iteration 30 	loss: 345679.440098 	improvement:  0.0
iteration 31 	loss: 345679.440098 	improvement:  0.0
iteration 32 	loss: 345679.440098 	improvement:  0.0
iteration 33 	loss: 345679.440098 	improvement:  0.0
iteration 34 	loss: 345679.440098 	improvement:  0.0
iteration 35 	loss: 345679.440098 	improvement:  0.0
iteration 36 	loss: 345679.440098 	improvement:  0.0
iteration 37 	loss: 345679.440098 	improvement:  0.0
iteration 38 	loss: 345679.440098 	improvement:  0.0
iteration 39 	loss: 345679.440098 	improvement:  0.0
iteration 40 	loss: 345679.440098 	improvement:  0.0
iteration 41 	loss: 345679.440098 	improvement:  0.0
iteration 42 	loss: 345679.440098 	improvement:  0.0
iteration 43 	loss: 345679.440098 	improvement:  0.0
iteration 44 	loss: 345679.440098 	improvement:  0.0
iteration 45 	loss: 345679.440098 	improvement:  0.0
iteration 46 	loss: 345679.440098 	improvement:  0.0
iteration 47 	loss: 345679.440098 	improvement:  0.0
iteration 48 	loss: 345679.440098 	improvement:  0.0
iteration 49 	loss: 345679.440098 	improvement:  0.0
iteration 50 	loss: 345679.440098 	improvement:  0.0
iteration 51 	loss: 345679.440098 	improvement:  0.0
iteration 52 	loss: 345679.440098 	improvement:  0.0
iteration 53 	loss: 345679.440098 	improvement:  0.0
iteration 54 	loss: 345679.440098 	improvement:  0.0
iteration 55 	loss: 345679.440098 	improvement:  0.0
iteration 56 	loss: 345679.440098 	improvement:  0.0
iteration 57 	loss: 345679.440098 	improvement:  0.0
iteration 58 	loss: 345679.440098 	improvement:  0.0
iteration 59 	loss: 345679.440098 	improvement:  0.0
iteration 60 	loss: 345679.440098 	improvement:  0.0
iteration 61 	loss: 345679.440098 	improvement:  0.0
iteration 62 	loss: 345679.440098 	improvement:  0.0
iteration 63 	loss: 345679.440098 	improvement:  0.0
iteration 64 	loss: 345679.440098 	improvement:  0.0
iteration 65 	loss: 345679.440098 	improvement:  0.0
iteration 66 	loss: 345679.440098 	improvement:  0.0
iteration 67 	loss: 345679.440098 	improvement:  0.0
iteration 68 	loss: 345679.440098 	improvement:  0.0
iteration 69 	loss: 345679.440098 	improvement:  0.0
iteration 70 	loss: 345679.440098 	improvement:  0.0
iteration 71 	loss: 345679.440098 	improvement:  0.0
iteration 72 	loss: 345679.440098 	improvement:  0.0
iteration 73 	loss: 345679.440098 	improvement:  0.0
iteration 74 	loss: 345679.440098 	improvement:  0.0
iteration 75 	loss: 345679.440098 	improvement:  0.0
iteration 76 	loss: 345679.440098 	improvement:  0.0
iteration 77 	loss: 345679.440098 	improvement:  0.0
iteration 78 	loss: 345679.440098 	improvement:  0.0
iteration 79 	loss: 345679.440098 	improvement:  0.0
iteration 80 	loss: 345679.440098 	improvement:  0.0
iteration 81 	loss: 345679.440098 	improvement:  0.0
iteration 82 	loss: 345679.440098 	improvement:  0.0
iteration 83 	loss: 345679.440098 	improvement:  0.0
iteration 84 	loss: 345679.440098 	improvement:  0.0
iteration 85 	loss: 345679.440098 	improvement:  0.0
iteration 86 	loss: 345679.440098 	improvement:  0.0
iteration 87 	loss: 345679.440098 	improvement:  0.0
iteration 88 	loss: 345679.440098 	improvement:  0.0
iteration 89 	loss: 345679.440098 	improvement:  0.0
iteration 90 	loss: 345679.440098 	improvement:  0.0
iteration 91 	loss: 345679.440098 	improvement:  0.0
iteration 92 	loss: 345679.440098 	improvement:  0.0
iteration 93 	loss: 345679.440098 	improvement:  0.0
iteration 94 	loss: 345679.440098 	improvement:  0.0
iteration 95 	loss: 345679.440098 	improvement:  0.0
iteration 96 	loss: 345679.440098 	improvement:  0.0
iteration 97 	loss: 345679.440098 	improvement:  0.0
iteration 98 	loss: 345679.440098 	improvement:  0.0
iteration 99 	loss: 345679.440098 	improvement:  0.0
iteration 100 	loss: 345679.440098 	improvement:  0.0
--- Execution Time ---
--- 114.082912207 seconds ---




### dense-svm

python bismarck_front.py dense-svm-spec.py
A shuffled table __bismarck_shuffled_forest_12 is created for training
iteration 1 	loss: 415731.592039 	improvement:  None
iteration 2 	loss: 414047.543357 	improvement:  0.00405080757407
iteration 3 	loss: 414476.211101 	improvement:  -0.00103531043875
iteration 4 	loss: 414832.432456 	improvement:  -0.0008594494594
iteration 5 	loss: 415155.937225 	improvement:  -0.000779844445277
iteration 6 	loss: 415389.849294 	improvement:  -0.000563431828746
iteration 7 	loss: 415580.789035 	improvement:  -0.000459663956969
iteration 8 	loss: 415684.550929 	improvement:  -0.000249679236165
iteration 9 	loss: 415730.369956 	improvement:  -0.000110225474612
iteration 10 	loss: 415785.245934 	improvement:  -0.000131998963099
iteration 11 	loss: 415816.29438 	improvement:  -7.46742379548e-05
iteration 12 	loss: 415848.463694 	improvement:  -7.73642445949e-05
iteration 13 	loss: 415887.575714 	improvement:  -9.40535390286e-05
iteration 14 	loss: 415886.370607 	improvement:  2.89767325448e-06
iteration 15 	loss: 415907.831641 	improvement:  -5.16031186419e-05
iteration 16 	loss: 415904.2413 	improvement:  8.63254100516e-06
iteration 17 	loss: 415894.398109 	improvement:  2.36669637205e-05
iteration 18 	loss: 415939.993957 	improvement:  -0.000109633233165
iteration 19 	loss: 415922.141541 	improvement:  4.29206524484e-05
iteration 20 	loss: 415892.996841 	improvement:  7.00724899451e-05
iteration 21 	loss: 415919.193479 	improvement:  -6.29888894717e-05
iteration 22 	loss: 415932.469222 	improvement:  -3.19190438074e-05
iteration 23 	loss: 415939.335714 	improvement:  -1.65086725614e-05
iteration 24 	loss: 415926.934192 	improvement:  2.98156990002e-05
iteration 25 	loss: 415888.490797 	improvement:  9.24282433997e-05
iteration 26 	loss: 415931.53092 	improvement:  -0.00010348957563
iteration 27 	loss: 415912.944124 	improvement:  4.4687153842e-05
iteration 28 	loss: 415911.946976 	improvement:  2.39749111226e-06
iteration 29 	loss: 415897.044749 	improvement:  3.5830245869e-05
iteration 30 	loss: 415937.896124 	improvement:  -9.82247316585e-05
iteration 31 	loss: 415889.407811 	improvement:  0.000116575849914
iteration 32 	loss: 415947.009469 	improvement:  -0.000138502344763
iteration 33 	loss: 415935.496066 	improvement:  2.76799755472e-05
iteration 34 	loss: 415944.723834 	improvement:  -2.21855753724e-05
iteration 35 	loss: 415853.911431 	improvement:  0.000218328055177
iteration 36 	loss: 415908.200497 	improvement:  -0.000130548405901
iteration 37 	loss: 415950.320679 	improvement:  -0.00010127278603
iteration 38 	loss: 415947.934865 	improvement:  5.73581491694e-06
iteration 39 	loss: 415921.387292 	improvement:  6.38242680195e-05
iteration 40 	loss: 415966.103166 	improvement:  -0.00010751039823
iteration 41 	loss: 415889.863719 	improvement:  0.000183282835923
iteration 42 	loss: 415931.281442 	improvement:  -9.95882002356e-05
iteration 43 	loss: 415927.842346 	improvement:  8.26842433217e-06
iteration 44 	loss: 415929.092998 	improvement:  -3.00689662403e-06
iteration 45 	loss: 415931.567214 	improvement:  -5.94864885541e-06
iteration 46 	loss: 415923.465816 	improvement:  1.94777182249e-05
iteration 47 	loss: 415953.170488 	improvement:  -7.14186002651e-05
iteration 48 	loss: 415906.872722 	improvement:  0.000111305236468
iteration 49 	loss: 415963.22071 	improvement:  -0.000135482222896
iteration 50 	loss: 415932.313038 	improvement:  7.43038574765e-05
iteration 51 	loss: 415855.626647 	improvement:  0.000184372285206
iteration 52 	loss: 415945.431518 	improvement:  -0.000215952042047
iteration 53 	loss: 415949.946853 	improvement:  -1.08555942507e-05
iteration 54 	loss: 415878.47947 	improvement:  0.000171817267252
iteration 55 	loss: 415928.488903 	improvement:  -0.000120250110354
iteration 56 	loss: 415918.054977 	improvement:  2.5085864593e-05
iteration 57 	loss: 415912.449067 	improvement:  1.34784016223e-05
iteration 58 	loss: 415951.704294 	improvement:  -9.43833901895e-05
iteration 59 	loss: 415913.39458 	improvement:  9.21013507326e-05
iteration 60 	loss: 415964.145761 	improvement:  -0.000122023434336
iteration 61 	loss: 415899.954167 	improvement:  0.000154320017002
iteration 62 	loss: 415905.05547 	improvement:  -1.22656993656e-05
iteration 63 	loss: 415901.670937 	improvement:  8.13775488531e-06
iteration 64 	loss: 415945.063783 	improvement:  -0.000104334386915
iteration 65 	loss: 415910.217807 	improvement:  8.3775428058e-05
iteration 66 	loss: 415912.427928 	improvement:  -5.31393629291e-06
iteration 67 	loss: 415875.409554 	improvement:  8.90052119564e-05
iteration 68 	loss: 415980.361011 	improvement:  -0.000252362739595
iteration 69 	loss: 415899.8343 	improvement:  0.000193582963778
iteration 70 	loss: 415876.391808 	improvement:  5.63657178138e-05
iteration 71 	loss: 415938.301216 	improvement:  -0.000148864925275
iteration 72 	loss: 415937.656119 	improvement:  1.55094345518e-06
iteration 73 	loss: 415857.059461 	improvement:  0.000193771004326
iteration 74 	loss: 415989.651962 	improvement:  -0.000318841528718
iteration 75 	loss: 415915.968566 	improvement:  0.00017712795473
iteration 76 	loss: 415933.529435 	improvement:  -4.22221564071e-05
iteration 77 	loss: 415933.245979 	improvement:  6.81493882966e-07
iteration 78 	loss: 415931.573211 	improvement:  4.02172108187e-06
iteration 79 	loss: 415921.651805 	improvement:  2.38534578689e-05
iteration 80 	loss: 415911.403552 	improvement:  2.46398627616e-05
iteration 81 	loss: 415922.742658 	improvement:  -2.72632706272e-05
iteration 82 	loss: 415943.78687 	improvement:  -5.05964461923e-05
iteration 83 	loss: 415921.298 	improvement:  5.40670902604e-05
iteration 84 	loss: 415936.386352 	improvement:  -3.62769405667e-05
iteration 85 	loss: 415908.454462 	improvement:  6.71542356872e-05
iteration 86 	loss: 415943.234102 	improvement:  -8.3623304989e-05
iteration 87 	loss: 415951.192955 	improvement:  -1.91344690465e-05
iteration 88 	loss: 415902.049393 	improvement:  0.000118147423631
iteration 89 	loss: 415914.385642 	improvement:  -2.9661429577e-05
iteration 90 	loss: 415988.726357 	improvement:  -0.000178740427185
iteration 91 	loss: 415873.509464 	improvement:  0.000276971190147
iteration 92 	loss: 415920.312268 	improvement:  -0.000112540958245
iteration 93 	loss: 415955.751967 	improvement:  -8.52079072954e-05
iteration 94 	loss: 415903.088631 	improvement:  0.000126608024308
iteration 95 	loss: 415917.274442 	improvement:  -3.41084535335e-05
iteration 96 	loss: 415919.375207 	improvement:  -5.05091960621e-06
iteration 97 	loss: 415884.396939 	improvement:  8.40986747098e-05
iteration 98 	loss: 415930.756419 	improvement:  -0.000111472035569
iteration 99 	loss: 415900.067608 	improvement:  7.37834625028e-05
iteration 100 	loss: 415938.28971 	improvement:  -9.19021299993e-05
--- Execution Time ---
--- 97.9622838497 seconds ---


### sparse-logit
python bismarck_front.py sparse-logit-spec.py
A shuffled table __bismarck_shuffled_dblife_22 is created for training
iteration 1 	loss: 17528.1296132 	improvement:  None
iteration 2 	loss: 15488.0754048 	improvement:  0.116387444265
iteration 3 	loss: 14284.7589732 	improvement:  0.077693089693
iteration 4 	loss: 13384.0053691 	improvement:  0.0630569690215
iteration 5 	loss: 12641.3743542 	improvement:  0.0554864552456
iteration 6 	loss: 11994.6865275 	improvement:  0.0511564493354
iteration 7 	loss: 11408.203278 	improvement:  0.0488952544276
iteration 8 	loss: 10862.848494 	improvement:  0.0478037400519
iteration 9 	loss: 10348.3883795 	improvement:  0.0473595958591
iteration 10 	loss: 9862.89440565 	improvement:  0.0469149355434
iteration 11 	loss: 9405.31182655 	improvement:  0.0463943504093
iteration 12 	loss: 8967.37165016 	improvement:  0.0465630682387
iteration 13 	loss: 8549.40423619 	improvement:  0.0466098016543
iteration 14 	loss: 8153.83576456 	improvement:  0.0462685423099
iteration 15 	loss: 7775.63107253 	improvement:  0.046383653406
iteration 16 	loss: 7416.50155881 	improvement:  0.0461865423362
iteration 17 	loss: 7079.60574437 	improvement:  0.0454251659981
iteration 18 	loss: 6763.13990713 	improvement:  0.0447010537974
iteration 19 	loss: 6462.54467319 	improvement:  0.044446106108
iteration 20 	loss: 6182.99193205 	improvement:  0.0432573785224
iteration 21 	loss: 5925.37709399 	improvement:  0.0416650775039
iteration 22 	loss: 5687.0896663 	improvement:  0.0402147279255
iteration 23 	loss: 5471.36805373 	improvement:  0.0379318113881
iteration 24 	loss: 5277.13754985 	improvement:  0.0354994403547
iteration 25 	loss: 5101.27838427 	improvement:  0.0333247265043
iteration 26 	loss: 4945.43163106 	improvement:  0.0305505289995
iteration 27 	loss: 4809.36939766 	improvement:  0.0275127114377
iteration 28 	loss: 4691.31814339 	improvement:  0.0245460983569
iteration 29 	loss: 4589.3236018 	improvement:  0.0217411265829
iteration 30 	loss: 4502.02342656 	improvement:  0.0190224492349
iteration 31 	loss: 4427.88566031 	improvement:  0.01646765448
iteration 32 	loss: 4365.15729871 	improvement:  0.0141666624696
iteration 33 	loss: 4312.20432355 	improvement:  0.0121308286358
iteration 34 	loss: 4267.76588395 	improvement:  0.0103052722594
iteration 35 	loss: 4230.49122344 	improvement:  0.00873399842581
iteration 36 	loss: 4199.38463182 	improvement:  0.00735295027921
iteration 37 	loss: 4173.50324614 	improvement:  0.00616313768464
iteration 38 	loss: 4151.94789283 	improvement:  0.00516481048082
iteration 39 	loss: 4133.97225062 	improvement:  0.0043294479301
iteration 40 	loss: 4119.03245875 	improvement:  0.00361390715006
iteration 41 	loss: 4106.57231908 	improvement:  0.00302501613938
iteration 42 	loss: 4096.21219916 	improvement:  0.0025228144354
iteration 43 	loss: 4087.56419445 	improvement:  0.00211121989884
iteration 44 	loss: 4080.32050275 	improvement:  0.00177212916007
iteration 45 	loss: 4074.28883099 	improvement:  0.00147823480026
iteration 46 	loss: 4069.23653561 	improvement:  0.0012400434989
iteration 47 	loss: 4064.99678481 	improvement:  0.00104190325758
iteration 48 	loss: 4061.42721665 	improvement:  0.000878123242275
iteration 49 	loss: 4058.42144913 	improvement:  0.000740076667756
iteration 50 	loss: 4055.87724579 	improvement:  0.000626894807999
iteration 51 	loss: 4053.72208339 	improvement:  0.000531367759324
iteration 52 	loss: 4051.88861651 	improvement:  0.000452292199357
iteration 53 	loss: 4050.32821142 	improvement:  0.000385105624447
iteration 54 	loss: 4048.99740911 	improvement:  0.000328566534235
iteration 55 	loss: 4047.85777052 	improvement:  0.000281461922876
iteration 56 	loss: 4046.88017488 	improvement:  0.000241509383344
iteration 57 	loss: 4046.040492 	improvement:  0.000207488941588
iteration 58 	loss: 4045.31665673 	improvement:  0.00017889966142
iteration 59 	loss: 4044.69089753 	improvement:  0.000154687321931
iteration 60 	loss: 4044.14859052 	improvement:  0.000134078726595
iteration 61 	loss: 4043.67766622 	improvement:  0.000116445845779
iteration 62 	loss: 4043.26720659 	improvement:  0.000101506514122
iteration 63 	loss: 4042.90930428 	improvement:  8.8518095061e-05
iteration 64 	loss: 4042.59602769 	improvement:  7.74879107969e-05
iteration 65 	loss: 4042.3216368 	improvement:  6.78749213774e-05
iteration 66 	loss: 4042.08079425 	improvement:  5.95802532008e-05
iteration 67 	loss: 4041.86904671 	improvement:  5.23857774964e-05
iteration 68 	loss: 4041.6823439 	improvement:  4.61921971277e-05
iteration 69 	loss: 4041.51747776 	improvement:  4.07914628692e-05
iteration 70 	loss: 4041.37165044 	improvement:  3.60823175435e-05
iteration 71 	loss: 4041.2426443 	improvement:  3.19213748816e-05
iteration 72 	loss: 4041.12821957 	improvement:  2.83142441995e-05
iteration 73 	loss: 4041.02664336 	improvement:  2.51356069099e-05
iteration 74 	loss: 4040.93631531 	improvement:  2.23527475025e-05
iteration 75 	loss: 4040.85591247 	improvement:  1.98970822123e-05
iteration 76 	loss: 4040.78427246 	improvement:  1.77289209023e-05
iteration 77 	loss: 4040.72040477 	improvement:  1.58057644367e-05
iteration 78 	loss: 4040.66337914 	improvement:  1.41127401669e-05
iteration 79 	loss: 4040.61242285 	improvement:  1.2610871134e-05
iteration 80 	loss: 4040.56686423 	improvement:  1.12751758823e-05
iteration 81 	loss: 4040.5261215 	improvement:  1.0083420881e-05
iteration 82 	loss: 4040.48965054 	improvement:  9.02629026603e-06
iteration 83 	loss: 4040.45700543 	improvement:  8.07949370332e-06
iteration 84 	loss: 4040.42775602 	improvement:  7.23913349182e-06
iteration 85 	loss: 4040.4015456 	improvement:  6.48704124973e-06
iteration 86 	loss: 4040.378052 	improvement:  5.8146691869e-06
iteration 87 	loss: 4040.35699066 	improvement:  5.212714773e-06
iteration 88 	loss: 4040.33809251 	improvement:  4.67734674276e-06
iteration 89 	loss: 4040.32113194 	improvement:  4.1978088546e-06
iteration 90 	loss: 4040.30590357 	improvement:  3.76909911429e-06
iteration 91 	loss: 4040.29222823 	improvement:  3.38472809648e-06
iteration 92 	loss: 4040.27994712 	improvement:  3.03966095172e-06
iteration 93 	loss: 4040.26891642 	improvement:  2.73018179303e-06
iteration 94 	loss: 4040.25900497 	improvement:  2.4531649093e-06
iteration 95 	loss: 4040.25009772 	improvement:  2.20462443097e-06
iteration 96 	loss: 4040.24209176 	improvement:  1.9815499255e-06
iteration 97 	loss: 4040.2348952 	improvement:  1.78122108446e-06
iteration 98 	loss: 4040.22842525 	improvement:  1.60137766931e-06
iteration 99 	loss: 4040.22260785 	improvement:  1.43987106858e-06
iteration 100 	loss: 4040.21737672 	improvement:  1.29476154109e-06
--- Execution Time ---
--- 2.34308481216 seconds ---



### sparse-svm

python bismarck_front.py sparse-svm-spec.py
A shuffled table __bismarck_shuffled_dblife_122 is created for training
iteration 1 	loss: 19785.3275112 	improvement:  None
iteration 2 	loss: 17236.1892948 	improvement:  0.128839829159
iteration 3 	loss: 15809.2212939 	improvement:  0.0827890652907
iteration 4 	loss: 14570.6971895 	improvement:  0.0783418791675
iteration 5 	loss: 13873.763903 	improvement:  0.0478311557409
iteration 6 	loss: 13096.3482427 	improvement:  0.0560349495435
iteration 7 	loss: 12709.9849571 	improvement:  0.0295016044526
iteration 8 	loss: 12058.0242393 	improvement:  0.051295160461
iteration 9 	loss: 12018.4544123 	improvement:  0.00328161779818
iteration 10 	loss: 11574.9413903 	improvement:  0.0369026670784
iteration 11 	loss: 11106.5944244 	improvement:  0.0404621457839
iteration 12 	loss: 10815.6881276 	improvement:  0.026192213887
iteration 13 	loss: 10611.0311553 	improvement:  0.0189222331384
iteration 14 	loss: 10260.3768556 	improvement:  0.0330462039587
iteration 15 	loss: 10102.2133299 	improvement:  0.0154149821096
iteration 16 	loss: 9756.50716022 	improvement:  0.0342208344237
iteration 17 	loss: 9564.16455442 	improvement:  0.0197142894116
iteration 18 	loss: 9258.05529291 	improvement:  0.0320058547467
iteration 19 	loss: 8945.65503811 	improvement:  0.0337436151458
iteration 20 	loss: 8679.95408129 	improvement:  0.029701677036
iteration 21 	loss: 8452.00010364 	improvement:  0.0262621179236
iteration 22 	loss: 8124.0692431 	improvement:  0.0387992021437
iteration 23 	loss: 7882.73680703 	improvement:  0.0297058566155
iteration 24 	loss: 7557.8942998 	improvement:  0.0412093559868
iteration 25 	loss: 7302.79933728 	improvement:  0.033752120948
iteration 26 	loss: 7003.04671837 	improvement:  0.0410462625442
iteration 27 	loss: 6741.144582 	improvement:  0.0373983134639
iteration 28 	loss: 6455.45643398 	improvement:  0.0423797686789
iteration 29 	loss: 6182.0206954 	improvement:  0.0423573052314
iteration 30 	loss: 5903.35175737 	improvement:  0.0450773220867
iteration 31 	loss: 5631.14589516 	improvement:  0.0461103917558
iteration 32 	loss: 5372.97688404 	improvement:  0.0458466209052
iteration 33 	loss: 5147.85387434 	improvement:  0.0418991212063
iteration 34 	loss: 4927.68429374 	improvement:  0.0427691978004
iteration 35 	loss: 4709.7822088 	improvement:  0.0442199767594
iteration 36 	loss: 4536.54451991 	improvement:  0.0367825264967
iteration 37 	loss: 4366.34569742 	improvement:  0.037517282536
iteration 38 	loss: 4233.0673171 	improvement:  0.0305240101335
iteration 39 	loss: 4108.23144743 	improvement:  0.0294906412595
iteration 40 	loss: 3994.69449984 	improvement:  0.0276364535534
iteration 41 	loss: 3904.11000426 	improvement:  0.0226762010435
iteration 42 	loss: 3825.20161127 	improvement:  0.0202116213181
iteration 43 	loss: 3756.6235709 	improvement:  0.01792795448
iteration 44 	loss: 3700.45734438 	improvement:  0.0149512522241
iteration 45 	loss: 3647.17097385 	improvement:  0.014399941839
iteration 46 	loss: 3607.83348057 	improvement:  0.0107857551933
iteration 47 	loss: 3569.33759915 	improvement:  0.0106700826503
iteration 48 	loss: 3538.28017441 	improvement:  0.00870117322259
iteration 49 	loss: 3511.47289119 	improvement:  0.00757635967133
iteration 50 	loss: 3489.11196592 	improvement:  0.00636796181021
iteration 51 	loss: 3468.80377951 	improvement:  0.0058204456058
iteration 52 	loss: 3450.08335377 	improvement:  0.00539679582077
iteration 53 	loss: 3434.55178003 	improvement:  0.00450179666743
iteration 54 	loss: 3420.32737849 	improvement:  0.0041415597852
iteration 55 	loss: 3408.48974153 	improvement:  0.00346096605838
iteration 56 	loss: 3398.18093896 	improvement:  0.00302444875818
iteration 57 	loss: 3388.69498194 	improvement:  0.00279148085316
iteration 58 	loss: 3380.95579165 	improvement:  0.00228382617227
iteration 59 	loss: 3373.73676043 	improvement:  0.00213520426166
iteration 60 	loss: 3367.82560137 	improvement:  0.00175211033986
iteration 61 	loss: 3362.43521739 	improvement:  0.0016005531811
iteration 62 	loss: 3357.44505306 	improvement:  0.00148409233318
iteration 63 	loss: 3353.31909813 	improvement:  0.00122889723058
iteration 64 	loss: 3349.68308256 	improvement:  0.00108430348221
iteration 65 	loss: 3346.34056851 	improvement:  0.000997859789128
iteration 66 	loss: 3343.33108113 	improvement:  0.000899336845731
iteration 67 	loss: 3340.72901058 	improvement:  0.000778286831437
iteration 68 	loss: 3338.37083571 	improvement:  0.00070588630998
iteration 69 	loss: 3336.2814017 	improvement:  0.000625884332697
iteration 70 	loss: 3334.48652794 	improvement:  0.000537986321832
iteration 71 	loss: 3332.83203676 	improvement:  0.000496175699939
iteration 72 	loss: 3331.4583772 	improvement:  0.000412159851442
iteration 73 	loss: 3330.11629886 	improvement:  0.000402850098577
iteration 74 	loss: 3328.9800082 	improvement:  0.000341216509936
iteration 75 	loss: 3327.93140233 	improvement:  0.000314993140591
iteration 76 	loss: 3327.03760864 	improvement:  0.000268573352697
iteration 77 	loss: 3326.21197966 	improvement:  0.000248157393408
iteration 78 	loss: 3325.49121999 	improvement:  0.000216690841644
iteration 79 	loss: 3324.85084502 	improvement:  0.000192565525665
iteration 80 	loss: 3324.26684821 	improvement:  0.000175646019145
iteration 81 	loss: 3323.74547932 	improvement:  0.000156837255532
iteration 82 	loss: 3323.30388767 	improvement:  0.000132859647012
iteration 83 	loss: 3322.89405315 	improvement:  0.000123321407326
iteration 84 	loss: 3322.51536098 	improvement:  0.000113964564016
iteration 85 	loss: 3322.17771372 	improvement:  0.000101623988887
iteration 86 	loss: 3321.89345871 	improvement:  8.55628529553e-05
iteration 87 	loss: 3321.61811891 	improvement:  8.28864036346e-05
iteration 88 	loss: 3321.39231427 	improvement:  6.79803128915e-05
iteration 89 	loss: 3321.16882516 	improvement:  6.72877783783e-05
iteration 90 	loss: 3320.98159065 	improvement:  5.63760900444e-05
iteration 91 	loss: 3320.80918782 	improvement:  5.19132137424e-05
iteration 92 	loss: 3320.65003681 	improvement:  4.79253718803e-05
iteration 93 	loss: 3320.52077517 	improvement:  3.89266072869e-05
iteration 94 	loss: 3320.39045372 	improvement:  3.92472931971e-05
iteration 95 	loss: 3320.27870896 	improvement:  3.36541021869e-05
iteration 96 	loss: 3320.17730898 	improvement:  3.05395981387e-05
iteration 97 	loss: 3320.08937124 	improvement:  2.64858564879e-05
iteration 98 	loss: 3320.01291845 	improvement:  2.30273300299e-05
iteration 99 	loss: 3319.94096853 	improvement:  2.16715761557e-05
iteration 100 	loss: 3319.87320331 	improvement:  2.04115739714e-05
--- Execution Time ---
--- 1.9930460453 seconds ---
